{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a code to extract the keywords (like Inheritance, encapsulation, multithreading) from the document shared in the link http://bit.ly/epo_keyword_extraction_document, and upload the code in Github and also mention the keywords in order of their weightages in a Google doc or excel sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #To work with regular expressions.\n",
    "import PyPDF2 #Python library to read, manipulate and extract PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Followed tutorial \"https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\" to learn how to extract data from pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize file pointer and a PdfFileReader object.\n",
    "filename ='JavaBasics-notes.pdf' \n",
    "pdfFileObj = open(filename, 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) #It initializes a PdfFileReader object.\n",
    "num_pages = pdfReader.numPages #method numPages return total number of pages in a pdf document.\n",
    "\n",
    "#Gather all the text in document in text form.\n",
    "count = 0\n",
    "text = \"\"\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count) #Retrieves a page by number from this PDF file, and returns a pageObject.\n",
    "    count = count + 1\n",
    "    text = text + pageObj.extractText() #It extracts the text and returns a unicode string object.\n",
    "    \n",
    "#PyPDF2 can not read scanned pdf files/images. And the document provided do contain some images ,\n",
    "#and might contain keywords. \n",
    "\n",
    "if text != \"\":\n",
    "    text = text\n",
    "else:\n",
    "    text = textract.process('http://bit.ly/epo_keyword_extraction_document', method='tesseract', language='eng')\n",
    "#Now text file contain text data from pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.encode('ascii', 'ignore').lower() #it encodes a unicode string to ascii and ignores errors,\n",
    "#and then lowercase all words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3410"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = str(text) #re can't work on byte object, thus convert byte-->string. \n",
    "keywords = re.findall(r'[a-zA-Z]\\w+',text)\n",
    "len(keywords)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(keywords)) #Remove repetitions of words, and returns unique set of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(set(keywords)), columns = ['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Two famous algorithms that I found for keyword extraction are:</h4>\n",
    "<li>Rapid Automatic Keyword Extraction(RAKE)</li>\n",
    "<li>Term frequency â€“ Inverse document frequency (TF-IDF)</li>\n",
    "<br>\n",
    "TF-IDF algorithm can work on a large context and can figure out the most relevant words.t is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. <br>\n",
    "<h4>source</h4><br>\n",
    "https://nzmattgrant.wordpress.com/2018/01/31/a-comparison-of-rake-and-tf-idf-algorithms-for-finding-keywords-in-text/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing TF-IDF for extracting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized <b>Term Frequency (TF)</b>, aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the <b>Inverse Document Frequency (IDF)</b>, computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "<li>TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: <br>\n",
    "\n",
    "<b>TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).</b><br><br><br>\n",
    "\n",
    "<li>IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important.<b> However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance.</b> Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:<br> \n",
    "\n",
    "<b>IDF(t) = log_e(Total number of documents / Number of documents with term t in it)</b><br>\n",
    "<br><br>\n",
    "Hence we can conclude TF-IDF algorithm can even filter out stop words.\n",
    "\n",
    "<li>The product of the TF and IDF scores of a term is called the TF*IDF weight of that term.The higher the TF*IDF score (weight), the rarer the term and vice versa.</li>\n",
    "\n",
    "Source(s) :- <br>\n",
    "<li>https://www.elephate.com/blog/what-is-tf-idf/</li>\n",
    "<li>http://www.tfidf.com/</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of single document<br>\n",
    "<b>idf = (number_of_documents)/(number_of_times_word_appeared)</b>\n",
    "<br>\n",
    "This will give uniqueness of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_weight(word,text,total_docs=1):\n",
    "    list_of_words = re.findall(word,text) #findall return a list containing all the appearances of 'word'.\n",
    "    number_of_times_word_appeared =len(list_of_words)\n",
    "    tf = number_of_times_word_appeared/float(len(text)) #this returns term frequency\n",
    "    idf = np.log((total_docs)/float(number_of_times_word_appeared))\n",
    "    tf_idf = tf*idf\n",
    "    return number_of_times_word_appeared,tf,idf ,tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: key_weight(x,text)[0])\n",
    "df['tf'] = df['keywords'].apply(lambda x: key_weight(x,text)[1])\n",
    "df['idf'] = df['keywords'].apply(lambda x: key_weight(x,text)[2])\n",
    "df['tf_idf'] = df['keywords'].apply(lambda x: key_weight(x,text)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('tf_idf', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Words in the document with a high tfidf score occur frequently in the document and provide the most information about that specific document.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing dataframe in excel sheet and csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo read excel files\\npip3 install xlrd\\ndf = pd.read_excel('keywords.xlsx')\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store dataframe in excel sheet.\n",
    "df.to_excel('keywords.xlsx', sheet_name='keywords_weight') #pip3 install openpyxl\n",
    "\n",
    "\n",
    "'''\n",
    "To read excel files\n",
    "pip3 install xlrd\n",
    "df = pd.read_excel('keywords.xlsx')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store dataframe in csv sheet\n",
    "df.to_csv('keywords.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
